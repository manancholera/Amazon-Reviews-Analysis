{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1588718502363_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-71-151.ec2.internal:20888/proxy/application_1588718502363_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-65-66.ec2.internal:8042/node/containerlogs/container_1588718502363_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7ffb555ac890>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "df = spark.read\\\n",
    "          .option(\"header\", \"true\")\\\n",
    "          .option(\"inferSchema\", \"true\")\\\n",
    "          .option(\"basePath\", \"hdfs:///hive/amazon-reviews-pds/parquet/\")\\\n",
    "          .parquet(\"hdfs:///hive/amazon-reviews-pds/parquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date', 'year', 'product_category']"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only limited number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Year filter\n",
    "columns_to_keep = ['customer_id', 'review_id', 'product_id', 'product_parent', \n",
    "                   'product_title', 'star_rating', 'helpful_votes', 'total_votes',\n",
    "                   'verified_purchase', 'review_date', 'year', 'product_category']\n",
    "df_limited = df.select(columns_to_keep).filter(F.col(\"year\")>2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_limited.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216"
     ]
    }
   ],
   "source": [
    "df_limited.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71519024"
     ]
    }
   ],
   "source": [
    "df_limited.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude Multiple Reviews and consider only one review for every customer_id, product_category and product_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+--------------------+-------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|verified_purchase|review_date|year|    product_category|row_num|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+--------------------+-------+\n",
      "|   10000034|R18KORVQBM4KTQ|B0096YOL56|     837621444|Trident Case AEGI...|          5|            0|          1|                Y| 2013-10-27|2013|            Wireless|      1|\n",
      "|   10000128|R10ZQ3KGV1I2XB|1421663260|     593136478|Golden Retrievers...|          4|            0|          0|                Y| 2011-01-04|2011|               Books|      1|\n",
      "|   10000166| RKNBFD2B5J2KL|B002DQL34G|     827931632|He-Man and the Ma...|          5|            1|          1|                Y| 2014-03-03|2014|           Video_DVD|      1|\n",
      "|   10000337| R561SQN54II24|B0000648ZP|     830751694|       Big Wednesday|          5|            0|          0|                Y| 2015-03-19|2015|           Video_DVD|      1|\n",
      "|   10000391|R1IFDIZYHYJB79|B00CQ35HBQ|     432392654|Kingston Technolo...|          5|           18|         20|                Y| 2014-10-29|2014|                  PC|      1|\n",
      "|   10000392| RPGI2AI6S0E2X|0789211343|      16303633|Wristwatch Annual...|          5|            0|          0|                Y| 2013-12-12|2013|               Books|      1|\n",
      "|   10000810|R3A54C9LEDSS7P|B00ESW9SWW|     212800988|Memory Card Carry...|          5|            0|          0|                Y| 2014-08-14|2014|                  PC|      1|\n",
      "|   10000971|R2TTNXGH75RD6H|B00G9MJLIU|       7176679|Jaynell's Wolf (A...|          4|            2|          2|                N| 2013-10-29|2013|Digital_Ebook_Pur...|      1|\n",
      "|   10001012|R1APK015FILZHN|B00FOILW5A|      76548106|      Grimm Season 3|          5|            0|          0|                Y| 2015-03-10|2015|Digital_Video_Dow...|      1|\n",
      "|   10001052|R274ZLVO49RYA3|0757902189|     792270473|James Bond 007 Co...|          1|            9|         24|                N| 2008-04-05|2008|               Books|      1|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+--------------------+-------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_final = df_limited.select(\"*\", F.row_number().over(Window.partitionBy(\"customer_id\",\"product_category\",\"product_id\").orderBy(\"customer_id\")).alias(\"row_num\")).where(\"row_num = 1\")\n",
    "\n",
    "df_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>"
     ]
    }
   ],
   "source": [
    "#Type\n",
    "type(df_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65911069"
     ]
    }
   ],
   "source": [
    "#Count records in the dataframe\n",
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[customer_id: string, review_id: string, product_id: string, product_parent: string, product_title: string, star_rating: int, helpful_votes: int, total_votes: int, verified_purchase: string, review_date: date, year: int, product_category: string]"
     ]
    }
   ],
   "source": [
    "#Drop column row_num\n",
    "df_final.drop(\"row_num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|verified_purchase|review_date|year|product_category|row_num|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "|   10000034|R18KORVQBM4KTQ|B0096YOL56|     837621444|Trident Case AEGI...|          5|            0|          1|                Y| 2013-10-27|2013|        Wireless|      1|\n",
      "|   10000128|R10ZQ3KGV1I2XB|1421663260|     593136478|Golden Retrievers...|          4|            0|          0|                Y| 2011-01-04|2011|           Books|      1|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "#Persist Dataframe - will keep the data in the memory as much as possible after first action\n",
    "df_final.persist()\n",
    "df_final.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Explore the dataset and provide analysis by product-category and year:\n",
    "\n",
    " 1. Number of reviews\n",
    " 2. Number of users\n",
    " 3. Average and Median review stars\n",
    " 4. Percentiles of length of the review. Use the following percentiles: [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    " 5. Percentiles for number of reviews per product. For example, 10% of books got 5 or less reviews. Use the following percentiles: [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "- Digital_Ebook_Purchase\n",
    "- Books\n",
    "- Wireless\n",
    "- PC\n",
    "- Mobile_Apps\n",
    "- Video_DVD\n",
    "- Digital_Video_Download\n",
    "\n",
    "6. Identify week number (each year has 52 weeks) for each year and product category with most positive reviews (4 and 5 star).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|    product_category|year|no-of-reviews|\n",
      "+--------------------+----+-------------+\n",
      "|Digital_Ebook_Pur...|2014|      6723857|\n",
      "|Digital_Ebook_Pur...|2015|      4609422|\n",
      "|Digital_Ebook_Pur...|2013|      4569668|\n",
      "|               Books|2014|      3540849|\n",
      "|            Wireless|2015|      3000790|\n",
      "|               Books|2013|      2965949|\n",
      "|               Books|2015|      2860728|\n",
      "|            Wireless|2014|      2834083|\n",
      "|                  PC|2014|      2008492|\n",
      "|                  PC|2015|      1886149|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#1. No of reviews in a given year for a particular product category sorted in descending order\n",
    "df_final.groupby(F.col(\"product_category\"), F.col(\"year\"))\\\n",
    ".agg(F.count(F.col(\"review_id\")).alias(\"no-of-reviews\"))\\\n",
    ".sort(\"no-of-reviews\", ascending =  False)\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----------+\n",
      "|    product_category|year|no-of-users|\n",
      "+--------------------+----+-----------+\n",
      "|Digital_Ebook_Pur...|2014|    6723857|\n",
      "|Digital_Ebook_Pur...|2015|    4609422|\n",
      "|Digital_Ebook_Pur...|2013|    4569668|\n",
      "|               Books|2014|    3540849|\n",
      "|            Wireless|2015|    3000790|\n",
      "|               Books|2013|    2965949|\n",
      "|               Books|2015|    2860728|\n",
      "|            Wireless|2014|    2834083|\n",
      "|                  PC|2014|    2008492|\n",
      "|                  PC|2015|    1886149|\n",
      "+--------------------+----+-----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#2. No of users in a given year for a particular product category sorted in descending order\n",
    "df_final.groupby(F.col(\"product_category\"), F.col(\"year\"))\\\n",
    ".agg(F.count(F.col(\"customer_id\")).alias(\"no-of-users\"))\\\n",
    ".sort(\"no-of-users\", ascending =  False)\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.0]"
     ]
    }
   ],
   "source": [
    "#3. Average and median review stars\n",
    "\n",
    "#median review stars\n",
    "colName = \"star_rating\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "\n",
    "df_final.stat.approxQuantile(\"star_rating\", quantileProbs, relError) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|    product_category|year|   avg_star_rating|\n",
      "+--------------------+----+------------------+\n",
      "|           Video_DVD|2015| 4.529908628845081|\n",
      "|               Books|2015| 4.497382134897131|\n",
      "|           Video_DVD|2014| 4.485465693220808|\n",
      "|               Books|2014| 4.473276606825086|\n",
      "|               Books|2013| 4.412498326842437|\n",
      "|           Video_DVD|2013| 4.409182937819165|\n",
      "|Digital_Ebook_Pur...|2015| 4.349698291889959|\n",
      "|Digital_Ebook_Pur...|2014|4.3328144843056595|\n",
      "|               Books|2012| 4.314687408696959|\n",
      "|Digital_Ebook_Pur...|2013| 4.301699817142077|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#average review stars\n",
    "df_final.groupby(F.col(\"product_category\"), F.col(\"year\"))\\\n",
    ".agg(F.avg(F.col(\"star_rating\")).alias(\"avg_star_rating\"))\\\n",
    ".sort(\"avg_star_rating\", ascending =  False)\\\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+----+-----------------+--------------------+----------------------------+-----------+----+----------------+-----------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|          product_title|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|                 review_body|review_date|year|product_category|length_of_reviews|\n",
      "+-----------+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+----+-----------------+--------------------+----------------------------+-----------+----+----------------+-----------------+\n",
      "|         US|   16868299|R1UQX4QQ4IU00P|B005I6EU7U|      90774528|   Verso Kindle Fire...|          5|            0|          0|   N|                Y|A Perfect Fit and...|        I purchased the P...| 2012-06-29|2012|              PC|              955|\n",
      "|         JP|   26929322|R138SH9Y2VTYOB|B00AFY8K7G|     764843002|Ozone Rage ST ゲーミ...|          3|            0|          1|   N|                Y|     音質はいいけど…|fpsプレイヤーです<br />足...| 2014-04-09|2014|              PC|              103|\n",
      "|         US|   42470542|R29LNYEZJ3YNT8|B007R1SIY0|     112516052|   CISCO SYSTEMS WAP...|          5|            0|          2|   N|                N| Gigh performance AP|        It is a wireless ...| 2012-06-29|2012|              PC|              138|\n",
      "|         US|   43829974|R3HL06XL1C5Z5S|B006HUMYCO|     569961969|   ASUS SDRW-08D2S-U...|          5|            0|          0|   N|                Y|                 wow|        Amazing product. ...| 2014-04-09|2014|              PC|               97|\n",
      "|         US|   14631907|R27HRAA98ZYIEK|B004WYA852|     536677419|   amFilm iPad 2 Scr...|          4|            0|          0|   N|                Y|Easy to apply, ha...|        I have been reluc...| 2012-06-29|2012|              PC|             1018|\n",
      "+-----------+-----------+--------------+----------+--------------+-----------------------+-----------+-------------+-----------+----+-----------------+--------------------+----------------------------+-----------+----+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[60.0, 121.0, 254.0, 656.0, 48091.0, 51019.0]"
     ]
    }
   ],
   "source": [
    "#4. Percentiles of length of reviews\n",
    "\n",
    "sample = df.withColumn(\"length_of_reviews\", F.length(F.col(\"review_body\")))\n",
    "sample.show(5)\n",
    "\n",
    "colname = \"length_of_reviews\"\n",
    "quantileProbs = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "relError = 0.05\n",
    "\n",
    "sample.stat.approxQuantile(colname, quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|    product_category|review_count|\n",
      "+--------------------+------------+\n",
      "|                  PC|     6897944|\n",
      "|            Wireless|     9002606|\n",
      "|Digital_Video_Dow...|     4115479|\n",
      "|Digital_Ebook_Pur...|    17923458|\n",
      "|               Books|    17134822|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[4115479.0, 5331078.0, 6897944.0, 17134822.0, 17923458.0, 17923458.0]"
     ]
    }
   ],
   "source": [
    "#5. Percentiles for number of reviews per product\n",
    "\n",
    "sample1 = df_final.groupby(F.col(\"product_category\"))\\\n",
    "                  .agg(F.count(F.col(\"review_id\")).alias(\"review_count\"))\n",
    "sample1.show(5)\n",
    "\n",
    "colname = \"review_count\"\n",
    "quantileProbs = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    "relError = 0.05\n",
    "\n",
    "sample1.stat.approxQuantile(colname, quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+-----------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|verified_purchase|review_date|year|product_category|row_num|week_number|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+-----------+\n",
      "|   10000034|R18KORVQBM4KTQ|B0096YOL56|     837621444|Trident Case AEGI...|          5|            0|          1|                Y| 2013-10-27|2013|        Wireless|      1|         44|\n",
      "|   10000128|R10ZQ3KGV1I2XB|1421663260|     593136478|Golden Retrievers...|          4|            0|          0|                Y| 2011-01-04|2011|           Books|      1|          2|\n",
      "|   10000166| RKNBFD2B5J2KL|B002DQL34G|     827931632|He-Man and the Ma...|          5|            1|          1|                Y| 2014-03-03|2014|       Video_DVD|      1|         10|\n",
      "|   10000337| R561SQN54II24|B0000648ZP|     830751694|       Big Wednesday|          5|            0|          0|                Y| 2015-03-19|2015|       Video_DVD|      1|         12|\n",
      "|   10000391|R1IFDIZYHYJB79|B00CQ35HBQ|     432392654|Kingston Technolo...|          5|           18|         20|                Y| 2014-10-29|2014|              PC|      1|         44|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+----+-----------+--------------------------+\n",
      "|    product_category|year|week_number|number_of_positive_reviews|\n",
      "+--------------------+----+-----------+--------------------------+\n",
      "|Digital_Ebook_Pur...|2015|          8|                    182021|\n",
      "|Digital_Ebook_Pur...|2014|          1|                    173042|\n",
      "|Digital_Ebook_Pur...|2015|         11|                    166548|\n",
      "|Digital_Ebook_Pur...|2015|          9|                    165343|\n",
      "|Digital_Ebook_Pur...|2015|         10|                    160535|\n",
      "|Digital_Ebook_Pur...|2015|         12|                    158036|\n",
      "|Digital_Ebook_Pur...|2015|         13|                    150086|\n",
      "|Digital_Ebook_Pur...|2014|         30|                    147866|\n",
      "|Digital_Ebook_Pur...|2014|         31|                    146918|\n",
      "|Digital_Ebook_Pur...|2014|         32|                    145727|\n",
      "+--------------------+----+-----------+--------------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#6. Identify week number of year and product category with most positive reviews\n",
    "\n",
    "df_weekno = df_final.withColumn(\"week_number\", F.date_format(F.to_date(\"review_date\", \"yyyy-MM-dd\"), \"w\"))\n",
    "df_weekno.show(5)\n",
    "\n",
    "df_weekno.groupby(F.col(\"product_category\"), F.col(\"year\"), F.col(\"week_number\"))\\\n",
    ".agg(F.count(F.expr(\"star_rating >= 4\")).alias(\"number_of_positive_reviews\"))\\\n",
    ".sort(\"number_of_positive_reviews\", ascending = False)\\\n",
    ".show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "1. Using Spark Pivot functionality, produce DataFrame with following columns:\n",
    "    1. Year\n",
    "    2. Month\n",
    "    3. Total number of reviews for \"Digital eBook Purchase\" category\n",
    "    4. Total number of reviews for \"Books\" category\n",
    "    5. Average stars for reviews for \"Digital eBook Purchase\" category\n",
    "    6. Average stars for reviews for \"Books\" category\n",
    "\n",
    "2. Produce two graphs to demonstrate aggregations from #1:\n",
    "    1. Number of reviews\n",
    "    2. Average stars\n",
    "\n",
    "3. Identify similar products (books) in both categories. Use \"product_title\" to match products. To account for potential differences in naming of products, compare titles after stripping spaces and converting to lower case.\n",
    "    1. Is there a difference in average rating for the similar books in digital and printed form?\n",
    "    2. To answer #1, you may calculate number of items with high stars in digital form versus printed form, and vise versa.    Alternatively, you can make the conclusion by using appropriate pairwise statistic.\n",
    "\n",
    "4. Using provided LDA starter notebook, perform LDA topic modeling for the reviews in Digital_Ebook_Purchase and Books categories. Consider reviews for the January of 2015 only.\n",
    "    1. Perform LDA separately for reviews with 1/2 stars and reviews with 4/5 stars.\n",
    "    2. Add stop words to the standard list as needed. In the example notebook, you can see some words like 34, br, p appear in the topics.\n",
    "    3. Identify 5 top topics for each case (1/2 versus 4/5)\n",
    "    4. Does topic modeling provides good approximation to number of stars given in the review?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------------------------+---------------------------------------+-----------------------+----------------------+\n",
      "|year|Digital_Ebook_Purchase_number_of_reviews|Digital_Ebook_Purchase_avg_review_stars|Books_number_of_reviews|Books_avg_review_stars|\n",
      "+----+----------------------------------------+---------------------------------------+-----------------------+----------------------+\n",
      "|2007|                                     508|                      3.938976377952756|                 761029|     4.258168873985091|\n",
      "|2015|                                 4609422|                      4.349698291889959|                2860728|     4.497382134897131|\n",
      "|2006|                                      36|                      4.027777777777778|                 568385|     4.196546355023443|\n",
      "|2013|                                 4569668|                      4.301699817142077|                2965949|     4.412498326842437|\n",
      "|2014|                                 6723857|                     4.3328144843056595|                3540849|     4.473276606825086|\n",
      "|2012|                                 1526589|                     4.2142574065449185|                1649726|     4.314687408696959|\n",
      "|2009|                                   31106|                     3.7771169549283097|                1015569|     4.246826163461074|\n",
      "|2005|                                      19|                     3.5789473684210527|                 521024|     4.148043084387667|\n",
      "|2010|                                  102515|                     3.8219675169487393|                1120766|    4.2469355779886255|\n",
      "|2011|                                  350131|                      4.055556348909408|                1303126|     4.251153764102627|\n",
      "+----+----------------------------------------+---------------------------------------+-----------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "DataFrame[year: int, Digital_Ebook_Purchase_number_of_reviews: bigint, Digital_Ebook_Purchase_avg_review_stars: double, Books_number_of_reviews: bigint, Books_avg_review_stars: double]"
     ]
    }
   ],
   "source": [
    "#1. Using Spark Pivot Functionality, produce a dataframe with relevant columns\n",
    "\n",
    "dfwithMonth = df_final.withColumn(\"month\", F.month(F.col(\"review_date\")))\n",
    "\n",
    "categories_to_pivot = ['Digital_Ebook_Purchase','Books']\n",
    "\n",
    "df_pivoted = dfwithMonth.groupby(F.col(\"year\")).pivot(\"product_category\", categories_to_pivot)\\\n",
    ".agg(F.count(F.col(\"review_id\")).alias(\"number_of_reviews\"),\n",
    "     F.avg(F.col(\"star_rating\")).alias(\"avg_review_stars\"))\n",
    "\n",
    "df_pivoted.show(10)\n",
    "df_pivoted.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Produce two graphs to demonstrate aggregations from #1\n",
    "\n",
    "df_pivoted.coalesce(1).write.csv(path = 'hdfs:////user/livy/df_pivoted.csv', header = 'true')\n",
    "#hdfs dfs -copyToLocal /user/livy/df_pivoted.csv ~/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "|customer_id|     review_id|product_id|product_parent|       product_title|star_rating|helpful_votes|total_votes|verified_purchase|review_date|year|product_category|row_num|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "|   10000034|R18KORVQBM4KTQ|B0096YOL56|     837621444|trident case aegi...|          5|            0|          1|                Y| 2013-10-27|2013|        Wireless|      1|\n",
      "|   10000128|R10ZQ3KGV1I2XB|1421663260|     593136478|golden retrievers...|          4|            0|          0|                Y| 2011-01-04|2011|           Books|      1|\n",
      "|   10000166| RKNBFD2B5J2KL|B002DQL34G|     827931632|he-man and the ma...|          5|            1|          1|                Y| 2014-03-03|2014|       Video_DVD|      1|\n",
      "|   10000337| R561SQN54II24|B0000648ZP|     830751694|       big wednesday|          5|            0|          0|                Y| 2015-03-19|2015|       Video_DVD|      1|\n",
      "|   10000391|R1IFDIZYHYJB79|B00CQ35HBQ|     432392654|kingston technolo...|          5|           18|         20|                Y| 2014-10-29|2014|              PC|      1|\n",
      "+-----------+--------------+----------+--------------+--------------------+-----------+-------------+-----------+-----------------+-----------+----+----------------+-------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#3. part-1\n",
    "\n",
    "for i in df_final.columns:\n",
    "    df_final = df_final.withColumn(i, F.ltrim(F.rtrim(df_final[i])))\n",
    "\n",
    "\n",
    "df_final = df_final.withColumn('product_title', F.lower(F.col('product_title')))\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. part-2\n",
    "\n",
    "df_books = df_final.select(\"*\").where(F.col('product_category').like('%Books%'))\n",
    "df_books = df_books.groupBy('product_title').agg(F.avg('star_rating'))\n",
    "\n",
    "df_ebooks = df_final.select(\"*\").where(F.col('product_category').like('%Digital_Ebook_Purchase%'))\n",
    "df_ebooks = df_ebooks.groupBy('product_title').agg(F.avg('star_rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|       product_title| avg(star_rating)|\n",
      "+--------------------+-----------------+\n",
      "|practice makes pe...|            4.625|\n",
      "|education and the...|              5.0|\n",
      "|existentialism: a...|              3.5|\n",
      "|dispatches from t...|4.534351145038168|\n",
      "|particular scanda...|4.583333333333333|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_books.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|       product_title| avg(star_rating)|\n",
      "+--------------------+-----------------+\n",
      "|         xavier grey|              3.0|\n",
      "|zombies ate my ne...|4.285714285714286|\n",
      "|ceo's expectant s...|            3.625|\n",
      "|simon: new orlean...|4.532846715328467|\n",
      "|deadlocked 1 (dea...| 4.40460947503201|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df_ebooks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+------------------+\n",
      "|       product_title| avg(star_rating)|       product_title|  avg(star_rating)|\n",
      "+--------------------+-----------------+--------------------+------------------+\n",
      "|\"rays of light\": ...|              5.0|\"rays of light\": ...|               5.0|\n",
      "|\"the siege of khe...|4.315789473684211|\"the siege of khe...| 3.326923076923077|\n",
      "|          'dem bon'z|              5.0|          'dem bon'z|               5.0|\n",
      "|   0400 roswell time|              5.0|   0400 roswell time|3.6666666666666665|\n",
      "|10 smart things g...|              4.8|10 smart things g...| 4.833333333333333|\n",
      "+--------------------+-----------------+--------------------+------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#3. part-3\n",
    "\n",
    "innerjoin = df_books.join(df_ebooks, df_books.product_title == df_ebooks.product_title)\n",
    "innerjoin.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference between average star ratings for the books in digital and printed form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Reviews LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 0 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_90 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_6 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_14 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_44 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_17 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_102 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_162 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_36 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_71 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_191 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_157 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_133 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_167 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_59 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_3 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_13 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_48 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_151 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_0 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_55 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_65 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_107 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_121 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_173 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_7 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_19 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_38 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_169 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_78 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_139 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_192 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_127 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_186 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_163 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_181 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_53 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_174 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_84 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_115 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_145 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_138 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_95 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_132 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_185 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_156 !\n",
      "20/05/05 23:45:15 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, ip-172-31-65-66.ec2.internal, 45421, None)\n",
      "20/05/05 23:45:15 INFO BlockManagerMaster: Removed executors on host ip-172-31-65-66.ec2.internal successfully.\n",
      "20/05/05 23:45:15 INFO DAGScheduler: Shuffle files lost for host: ip-172-31-65-66.ec2.internal (epoch 22)\n",
      "20/05/05 23:45:22 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-65-66.ec2.internal:45421 with 5.4 GB RAM, BlockManagerId(1, ip-172-31-65-66.ec2.internal, 45421, None)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_65 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_107 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_119 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_97 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_186 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_83 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_7 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_47 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_191 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_3 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 5.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_192 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_144 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_72 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_180 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_167 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_26 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.6 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_185 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_55 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_8 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_132 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_66 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_181 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_53 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_139 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_29 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_0 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_115 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_174 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_157 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_173 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_150 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_127 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_169 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_109 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_14 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_71 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_156 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_44 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_19 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_77 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_78 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_84 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_41 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_162 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_102 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_151 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_89 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_197 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_121 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 3.5 GB)\n"
     ]
    }
   ],
   "source": [
    "#Import ML Libraries\n",
    "\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import CountVectorizer, IDF,RegexTokenizer, Tokenizer\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import struct\n",
    "import re\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 0 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_90 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_6 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_14 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_44 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_17 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_102 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_162 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_36 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_71 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_191 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_157 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_133 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_167 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_59 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_3 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_13 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_48 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_151 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_0 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_55 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_65 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_107 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_121 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_173 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_7 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_19 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_38 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_169 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_78 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_139 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_192 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_127 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_186 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_163 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_181 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_53 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_174 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_84 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_115 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_145 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_138 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_95 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_132 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_185 !\n",
      "20/05/05 23:45:15 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_39_156 !\n",
      "20/05/05 23:45:15 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, ip-172-31-65-66.ec2.internal, 45421, None)\n",
      "20/05/05 23:45:15 INFO BlockManagerMaster: Removed executors on host ip-172-31-65-66.ec2.internal successfully.\n",
      "20/05/05 23:45:15 INFO DAGScheduler: Shuffle files lost for host: ip-172-31-65-66.ec2.internal (epoch 22)\n",
      "20/05/05 23:45:22 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-65-66.ec2.internal:45421 with 5.4 GB RAM, BlockManagerId(1, ip-172-31-65-66.ec2.internal, 45421, None)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_65 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_107 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_119 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_97 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_186 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_83 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_7 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_47 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 5.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_191 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 5.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_3 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 5.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_192 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_144 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_72 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_180 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_167 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_26 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.6 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_185 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_55 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_8 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_132 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_66 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_181 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_53 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_139 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_29 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_0 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.4 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_115 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_174 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.3 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_157 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_173 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_150 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 4.2 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_127 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_169 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.1 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_109 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_14 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_71 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 4.0 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_156 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_44 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.9 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_19 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_77 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_78 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.8 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_84 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_41 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_162 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.7 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_102 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_151 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.6 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_89 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.7 MB, free: 3.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_197 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.8 MB, free: 3.5 GB)\n",
      "20/05/05 23:45:22 INFO BlockManagerInfo: Added rdd_39_121 in memory on ip-172-31-65-66.ec2.internal:45421 (size: 39.9 MB, free: 3.5 GB)\n"
     ]
    }
   ],
   "source": [
    "#For star rating 4 and 5\n",
    "\n",
    "df_ml = df.filter((F.col(\"product_category\")==\"Digital_Ebook_Purchase\") | (F.col(\"product_category\")==\"Books\"))\n",
    "                   & (F.col(\"year\")==2015) \\\n",
    "                   & (F.col(\"review_date\")<'2015-02-01')\n",
    "                   & (F.col(\"star_rating\")>3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new DF with only narrative and unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import monotonically_increasing_id, concat\n",
    "\n",
    "df1 = df_ml.withColumn('review_text', \n",
    "                       F.concat(F.col('review_headline'),F.lit(' '), F.col('review_body')))\n",
    "corpus =df1.select('review_text')\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "corpus_df = corpus.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "# Remove records with no review text\n",
    "corpus_df = corpus_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.persist()\n",
    "print('Corpus size:', corpus_df.count())\n",
    "corpus_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Narrative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "'''\n",
    "tokenized_df = tokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show() \n",
    "'''\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", \n",
    "                                outputCol=\"words\",pattern=\"\\\\w+\", gaps=False)\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False) pattern=\"\\\\W\"\n",
    "\n",
    "tokenized_df = regexTokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(F.col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '', 'm', 'ich', 'y', 'zu']\n",
    "stop_words = stop_words + ['br','book','34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tokenized_df1 = remover.transform(tokenized_df)\n",
    "tokenized_df1.show(5)\n",
    "\n",
    "stopwordList = stop_words\n",
    "\n",
    "remover=StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered_more\" ,stopWords=stopwordList)\n",
    "tokenized_df2 = remover.transform(tokenized_df1)\n",
    "tokenized_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize (convert to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"filtered_more\", outputCol=\"features\", vocabSize = 10000)\n",
    "cvmodel = cv.fit(tokenized_df2)\n",
    "featurized_df = cvmodel.transform(tokenized_df2)\n",
    "vocab = cvmodel.vocabulary\n",
    "featurized_df.select('filtered_more','features','id').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is our DF to train LDA model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = featurized_df.select('features','id')\n",
    "countVectors.persist()\n",
    "print('Records in the DF:', countVectors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=10 means 10 words per topic\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(countVectors)\n",
    "\n",
    "\"\"\"\n",
    "ll = model.logLikelihood(countVectors)\n",
    "lp = model.logPerplexity(countVectors)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(countVectors)\n",
    "transformed.show(truncate=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display words for top 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print (\"topic: \", idx)\n",
    "    print (\"----------\")\n",
    "    for word in topic:\n",
    "       print (word)\n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For star rating 1 and 2\n",
    "\n",
    "df_ml = df.filter((F.col(\"product_category\")==\"Digital_Ebook_Purchase\") | (F.col(\"product_category\")==\"Books\"))\n",
    "                   & (F.col(\"year\")==2015) \\\n",
    "                   & (F.col(\"review_date\")<'2015-02-01')\n",
    "                   & (F.col(\"star_rating\")<3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new DF with only narrative and unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import monotonically_increasing_id, concat\n",
    "\n",
    "df1 = df_ml.withColumn('review_text', \n",
    "                       F.concat(F.col('review_headline'),F.lit(' '), F.col('review_body')))\n",
    "corpus =df1.select('review_text')\n",
    "\n",
    "# This will return a new DF with all the columns + id\n",
    "corpus_df = corpus.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "# Remove records with no review text\n",
    "corpus_df = corpus_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.persist()\n",
    "print('Corpus size:', corpus_df.count())\n",
    "corpus_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Narrative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\")\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "'''\n",
    "tokenized_df = tokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show() \n",
    "'''\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", \n",
    "                                outputCol=\"words\",pattern=\"\\\\w+\", gaps=False)\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False) pattern=\"\\\\W\"\n",
    "\n",
    "tokenized_df = regexTokenizer.transform(corpus_df)\n",
    "tokenized_df.select(\"review_text\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(F.col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', '', 'm', 'ich', 'y', 'zu']\n",
    "stop_words = stop_words + ['br','book','34']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tokenized_df1 = remover.transform(tokenized_df)\n",
    "tokenized_df1.show(5)\n",
    "\n",
    "stopwordList = stop_words\n",
    "\n",
    "remover=StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered_more\" ,stopWords=stopwordList)\n",
    "tokenized_df2 = remover.transform(tokenized_df1)\n",
    "tokenized_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize (convert to numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \n",
    "cv = CountVectorizer(inputCol=\"filtered_more\", outputCol=\"features\", vocabSize = 10000)\n",
    "cvmodel = cv.fit(tokenized_df2)\n",
    "featurized_df = cvmodel.transform(tokenized_df2)\n",
    "vocab = cvmodel.vocabulary\n",
    "featurized_df.select('filtered_more','features','id').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is our DF to train LDA model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = featurized_df.select('features','id')\n",
    "countVectors.persist()\n",
    "print('Records in the DF:', countVectors.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k=10 means 10 words per topic\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(countVectors)\n",
    "\n",
    "\"\"\"\n",
    "ll = model.logLikelihood(countVectors)\n",
    "lp = model.logPerplexity(countVectors)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(countVectors)\n",
    "transformed.show(truncate=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display words for top 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print (\"topic: \", idx)\n",
    "    print (\"----------\")\n",
    "    for word in topic:\n",
    "       print (word)\n",
    "    print (\"----------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
